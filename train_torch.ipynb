{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0f83442-fba0-48a6-9ec6-b93c6ee07d68",
   "metadata": {},
   "source": [
    "What do we want to know: if we train a model with synthetic data, does the quality of the output decrease like it should according to \"The Curse of Recursion: Training on Generated Data Makes Models Forget\" by Shumailov et al?  What if we train tiny models on separate subsets of the data and then have them generate data for a new big model?\n",
    "\n",
    "We don't know if text on the internet is generated via LLMs or not, so our problem shouldn't assume we have \"real\" data.  Instead, what if we cluster it and expose small samples from each different sub-cluster to different generative models?  Will the sum of the different probability distributes adequately capture the nuance we need?\n",
    "\n",
    "https://arxiv.org/pdf/2305.17493.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ce21d0-f6bc-49bc-ab62-4c63089aeadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install huggingface-hub huggingface-cli datasets accelerate evaluate\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy\n",
    "#from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a12d453-a7a2-43e7-9936-209b9ed5b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc3eba7-3acd-4871-9a10-738393ad464f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8e778-8743-4b67-a931-43ce04c8fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # Can we do higher on my machine?\n",
    "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for x,y in train_dataloader:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd5e8fd-ce51-4ebd-bf15-9ef6a45cff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TinyMNISTClassifier(nn.Module):\n",
    "    def __init__(self, outputs: int = 10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, 3), # 9x9 for valid.\n",
    "            nn.Conv2d(16, 32, 3, 3), # 3x3 for valid.\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Flatten(), # 32x3x3 -> 288\n",
    "            nn.Linear(288, outputs)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x) # No softmax output.\n",
    "\n",
    "\"\"\"\n",
    "epsilon = Normal(0, 1).sample((batch, dim)).to(z_mean.device)\n",
    "return z_mean + torch.exp(0.5 * z_log_var) * epsilon\n",
    "\"\"\"\n",
    "\n",
    "class TinyMNISTGenerator(nn.Module):\n",
    "    def __init__(self, latent_size: int):\n",
    "        super().__init__()\n",
    "        # TODO: This is stupid and we should make a better generator.\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_size, 28*28),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(28*28, 28*28),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(28*28, 28*28),\n",
    "        )\n",
    "\n",
    "    def sample_latent(self, z_mean, z_log_var):\n",
    "        return torch.normal(z_mean, z_log_var)\n",
    "        #return z_mean + torch.exp(z_log_var) * epsilon\n",
    "\n",
    "    def generate(self, z_sample):\n",
    "        return torch.reshape(self.net(z_sample), (-1, 28, 28))\n",
    "\n",
    "    def forward(self, z_mean, z_log_var):\n",
    "        # Alias for generate(sample_latent(z_mean, z_log_var))\n",
    "        return self.generate(self.sample_latent(z_mean, z_log_var))\n",
    "\n",
    "class TinyMNISTVAE(nn.Module):\n",
    "    def __init__(self, latent_size: int=30):\n",
    "        self.z_mean_encoder = TinyMNISTClassifier(latent_size)  # We could maybe do this all in one fell swoop?\n",
    "        self.z_log_var_encoder = TinyMNISTClassifier(latent_size)\n",
    "        self.decoder = TinyMNISTGenerator(latent_size)\n",
    "    def forward(self, x):\n",
    "        z_m = self.z_mean_encoder(x)\n",
    "        z_std = self.z_log_var_encoder(x)\n",
    "        return self.decoder(z_m, z_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb12e18-09f2-4632-a51b-ad7d17d911f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyMNISTClassifier(10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d286a38d-ba8c-4472-9927-f3b4c0fb19a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(params=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53863cd2-9cb5-4e4a-a213-1c79d50b5d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    batch_losses = list()\n",
    "    for batch_idx, (example, target) in enumerate(dataloader):\n",
    "        example = example.to(device)\n",
    "        target = target.to(device)\n",
    "        prediction = model(example)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_value = loss.item()\n",
    "        batch_losses.append(loss_value)\n",
    "        running_loss = 0.9*running_loss + 0.1*loss_value\n",
    "        if batch_idx % 100 == 0:\n",
    "            batch_losses = numpy.asarray(batch_losses)\n",
    "            print(f\"Batch ({batch_idx}) loss: Mean: {batch_losses.mean()} \\t Stddev: {batch_losses.std()} \\t Running: {running_loss}\")\n",
    "            batch_losses = list()\n",
    "\n",
    "def test(data_loader, model, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_count = 0\n",
    "    total_examples = 0\n",
    "    confusion_matrix = numpy.zeros((10, 10), dtype=numpy.uint8)\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in data_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(x)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            pred = pred.argmax(1).cpu().detach().numpy()\n",
    "            for predicted, correct in zip(pred, y):\n",
    "                total_examples += 1\n",
    "                confusion_matrix[predicted][correct] += 1\n",
    "                if predicted == correct:\n",
    "                    correct_count += 1\n",
    "    return test_loss, correct_count, correct_count / float(total_examples), confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5741a4cc-379f-4265-8acc-204af594678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(train_dataloader, model, loss_fn, opt)\n",
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0467d1d5-1bea-4555-b37d-3410517c06d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image # For visualizing the confusion matrix.\n",
    "\n",
    "for epoch_idx in range(0, 10):\n",
    "    train(train_dataloader, model, loss_fn, opt)\n",
    "    _, _, correct_percent, confusion_matrix = test(test_dataloader, model, loss_fn)\n",
    "    confusion_matrix = Image.fromarray(confusion_matrix)\n",
    "    display(confusion_matrix.resize((100, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752746f7-75db-4b9d-8ef5-58091c11b9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
